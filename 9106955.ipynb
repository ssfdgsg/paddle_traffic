{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import os\n",
    "import random\n",
    "import paddle\n",
    "import zipfile\n",
    "from paddle.io import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 来源：百度飞桨平台的公开数据集中的交通指示牌分类数据集。\n",
    "### **包含了生活中常见的58种交通信号标志**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/9e8295382c08452a95ae9e1ee03a8a69a7e072c35fbe4244818cec25979f2416)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/38ee23446e344cda85b221a4b2850950aedd29b7b68f4389a7dad7c3f0b3dcbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg') \n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from paddle.vision import transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),  # 首先调整到固定大小\n",
    "    transforms.RandomCrop(224),  # 然后随机裁剪\n",
    "    transforms.RandomHorizontalFlip(prob=0.5),  # 水平翻转\n",
    "    transforms.RandomRotation(15),  # 随机旋转，增强模型鲁棒性\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),  # 色彩扰动\n",
    "    transforms.RandomErasing(prob=0.2, scale=(0.02, 0.2)),  # 随机擦除\n",
    "    transforms.Transpose(),  # CHW格式转换\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
    "])\n",
    "\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),  # 首先调整到固定大小\n",
    "    transforms.CenterCrop(224),  # 然后中心裁剪\n",
    "    transforms.Transpose(),  # CHW格式转换\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数: 2965\n",
      "验证集样本数: 811\n",
      "测试集数量:394\n",
      "类别数: 58\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import paddle\n",
    "from paddle.vision import transforms\n",
    "\n",
    "\n",
    "# 定义用于ImageNet格式数据的自定义数据集\n",
    "class ImageNetDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data_dir, file_list, label_list, transforms=None, shuffle=False):\n",
    "        super(ImageNetDataset, self).__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # 读取文件列表\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        with open(file_list, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:  # 确保行格式正确\n",
    "                    self.file_paths.append(parts[0])\n",
    "                    self.labels.append(int(parts[1]))\n",
    "                    \n",
    "        # 读取标签映射\n",
    "        self.label_dict = {}\n",
    "        try:\n",
    "            with open(label_list, 'r') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    self.label_dict[i] = line.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"警告: 无法读取标签文件: {e}\")\n",
    "            # 如果无法读取标签文件，创建默认标签\n",
    "            for i in set(self.labels):\n",
    "                self.label_dict[i] = str(i)\n",
    "                \n",
    "        # 验证标签范围\n",
    "        max_label = max(self.labels) if self.labels else 0\n",
    "        if max_label >= len(self.label_dict):\n",
    "            print(f\"警告: 最大标签值 {max_label} 超出标签字典大小 {len(self.label_dict)}\")\n",
    "            # 扩展标签字典以包含所有标签\n",
    "            for i in range(len(self.label_dict), max_label + 1):\n",
    "                self.label_dict[i] = str(i)\n",
    "                \n",
    "        if shuffle:\n",
    "            import random\n",
    "            combined = list(zip(self.file_paths, self.labels))\n",
    "            random.shuffle(combined)\n",
    "            self.file_paths, self.labels = zip(*combined)\n",
    "            self.file_paths = list(self.file_paths)  # 转回列表\n",
    "            self.labels = list(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.data_dir, self.file_paths[index])\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"加载图像出错 {img_path}: {e}\")\n",
    "            # 返回一个黑色图像作为替代\n",
    "            img = Image.new('RGB', (64, 64), color='black')\n",
    "            \n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "        return img, self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "# 数据目录和文件路径\n",
    "data_dir = './dataset/traffic_Data/DATA'\n",
    "train_list = os.path.join(data_dir, 'train_list.txt')\n",
    "val_list = os.path.join(data_dir, 'val_list.txt')\n",
    "label_list = os.path.join(data_dir, 'labels.txt')\n",
    "test_list = os.path.join(data_dir,'test_list.txt')\n",
    "\n",
    "# 定义训练数据集\n",
    "train_dataset = ImageNetDataset(\n",
    "    data_dir=data_dir,\n",
    "    file_list=train_list,\n",
    "    label_list=label_list,\n",
    "    transforms=train_transforms,\n",
    "    shuffle=True)\n",
    "\n",
    "# 定义评估数据集\n",
    "eval_dataset = ImageNetDataset(\n",
    "    data_dir=data_dir,\n",
    "    file_list=val_list,\n",
    "    label_list=label_list,\n",
    "    transforms=eval_transforms)\n",
    "\n",
    "test_dataset = ImageNetDataset(\n",
    "    data_dir=data_dir,\n",
    "    file_list=test_list,\n",
    "    label_list=label_list,\n",
    "    transforms=eval_transforms)\n",
    "\n",
    "# 将数据集整合到字典中便于访问\n",
    "datasets = {\n",
    "    'train': train_dataset,\n",
    "    'eval': eval_dataset,\n",
    "    'test':test_dataset\n",
    "}\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 64\n",
    "loaders = {\n",
    "    k: paddle.io.DataLoader(\n",
    "        v, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=(k=='train')\n",
    "    ) for k, v in datasets.items()\n",
    "}\n",
    "\n",
    "# 打印数据集信息\n",
    "print(f\"训练集样本数: {len(train_dataset)}\")\n",
    "print(f\"验证集样本数: {len(eval_dataset)}\")\n",
    "print(f\"测试集数量:{len(test_dataset)}\")\n",
    "print(f\"类别数: {len(train_dataset.label_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用本数据集在V100上训练，模型的训练过程预估为5分钟左右；如无GPU，则预估为30分钟左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, random, math\n",
    "import paddle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from paddle.vision import transforms\n",
    "from paddle.vision.models import ShuffleNetV2\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 设置环境\n",
    "sys.path.append('external-libraries')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# 创建保存目录\n",
    "save_dir = 'work/mymodel'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据目录\n",
    "data_dir = './dataset/traffic_Data/DATA'\n",
    "\n",
    "# 注意: 此处缺少训练集和验证集的具体创建代码\n",
    "# 假设train_dataset和eval_dataset已经在其他地方定义\n",
    "# 如果没有，需要添加数据加载和预处理代码\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 64\n",
    "loaders = {\n",
    "    k: paddle.io.DataLoader(\n",
    "        v, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=(k=='train')\n",
    "    ) for k, v in datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineDecay:\n",
    "    def __init__(self, base_lr, warmup_steps, warmup_start_lr, total_steps):\n",
    "        self.base_lr = base_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            return self.warmup_start_lr + (self.base_lr - self.warmup_start_lr) * \\\n",
    "                   (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            return self.base_lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "# 计算总步数和训练配置\n",
    "num_epochs = 100\n",
    "steps_per_epoch = math.ceil(len(datasets['train']) / batch_size)\n",
    "total_steps = steps_per_epoch * num_epochs\n",
    "\n",
    "# 学习率调度参数\n",
    "base_lr = 0.004\n",
    "warmup_start_lr = 0.001\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "lr_scheduler = WarmupCosineDecay(base_lr, warmup_steps, warmup_start_lr, total_steps)\n",
    "\n",
    "# 配置优化器和损失函数\n",
    "loss_fn = paddle.nn.CrossEntropyLoss(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\paddle\\nn\\layer\\layers.py:2084: UserWarning: Skip loading for classifier.3.weight. classifier.3.weight receives a shape [1024, 1000], but the expected shape is [1024, 58].\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\paddle\\nn\\layer\\layers.py:2084: UserWarning: Skip loading for classifier.3.bias. classifier.3.bias receives a shape [1000], but the expected shape is [58].\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\n"
     ]
    }
   ],
   "source": [
    "from paddle.vision import mobilenet_v3_small\n",
    "\n",
    "# 直接使用预训练的MobileNetV3模型\n",
    "num_classes = len(datasets['train'].label_dict)\n",
    "model = mobilenet_v3_small(num_classes=num_classes, pretrained=True)\n",
    "\n",
    "# 创建优化器\n",
    "from paddle.optimizer.lr import LambdaDecay\n",
    "\n",
    "# 包装自定义学习率调度器\n",
    "def lr_lambda(epoch):\n",
    "    # 这里假设每调用一次返回当前学习率\n",
    "    return lr_scheduler.step() / base_lr\n",
    "\n",
    "scheduler = LambdaDecay(learning_rate=base_lr, lr_lambda=lr_lambda)\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(\n",
    "    learning_rate=scheduler,\n",
    "    parameters=model.parameters()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化训练记录\n",
    "records = {'epochs':[], 'train_losses':[], 'val_accs':[], 'lrs':[]}\n",
    "\n",
    "# 绘图配置\n",
    "def setup_plots():\n",
    "    plt.ion()  # 交互模式\n",
    "    fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    fig.tight_layout(pad=5)\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # 配置上半部分图表\n",
    "    ax1.set_title('loss&acc', fontsize=15)\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color='blue')\n",
    "    ax2.set_ylabel('Accuracy', color='red')\n",
    "    ax2.set_ylim([0, 1.0])\n",
    "    \n",
    "    # 配置下半部分图表\n",
    "    ax3.set_title('lr', fontsize=15)\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    \n",
    "    # 创建线条对象\n",
    "    lines = {\n",
    "        'loss': ax1.plot([], [], 'b-', label='Training Loss')[0],\n",
    "        'acc': ax2.plot([], [], 'r-', label='Validation Accuracy')[0],\n",
    "        'lr': ax3.plot([], [], 'g-', label='Learning Rate')[0]\n",
    "    }\n",
    "    \n",
    "    # 添加图例和网格\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax3.legend(loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    return fig, ax1, ax2, ax3, lines\n",
    "\n",
    "# 更新图表函数\n",
    "def update_plots(records, lines, ax1, ax2, ax3):\n",
    "    for k, line in lines.items():\n",
    "        data_key = 'train_losses' if k == 'loss' else ('val_accs' if k == 'acc' else 'lrs')\n",
    "        line.set_data(records['epochs'], records[data_key])\n",
    "    \n",
    "    if records['epochs']:\n",
    "        ax1.set_xlim(0, max(records['epochs']) + 1)\n",
    "        ax1.set_ylim(0, max(records['train_losses']) * 1.1)\n",
    "        ax3.set_xlim(0, max(records['epochs']) + 1)\n",
    "        ax3.set_ylim(0, max(records['lrs']) * 1.1)\n",
    "    \n",
    "    fig = ax1.figure\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    plt.savefig(f'{save_dir}/training_curves_latest.png')\n",
    "\n",
    "# 初始化图表\n",
    "fig, ax1, ax2, ax3, lines = setup_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3100, LR: 0.001252\n",
      "Epoch 66/100, Loss: 0.7453, Val Loss: 0.7611, Acc: 0.9889, LR: 0.001249, Time: 14.4s\n",
      "No improvement. Patience: 40/40\n",
      "Early stopping at epoch 66\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# 训练设置\n",
    "best_acc = 0.0\n",
    "patience_counter = 0\n",
    "patience = 40  # 早停参数\n",
    "\n",
    "# 训练循环\n",
    "step_count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for x_data, y_data in loaders['train']():\n",
    "        # 更新学习率\n",
    "        current_lr = lr_scheduler.step()\n",
    "        # 前向传播和反向传播\n",
    "        logits = model(x_data)\n",
    "        loss = loss_fn(logits, y_data)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 应用梯度更新模型参数\n",
    "        optimizer.step()        # 添加这行：执行优化步骤\n",
    "        optimizer.clear_grad()  # 添加这行：清除梯度\n",
    "        \n",
    "        # 累计损失\n",
    "        total_loss += loss.numpy()\n",
    "        batch_count += 1\n",
    "        step_count += 1\n",
    "        \n",
    "        # 打印进度\n",
    "        if step_count % 50 == 0:\n",
    "            print(f'Step: {step_count}, LR: {current_lr:.6f}')\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    val_batch_count = 0\n",
    "    \n",
    "    with paddle.no_grad():\n",
    "        for x_data, y_data in loaders['eval']():\n",
    "            # 前向传播\n",
    "            logits = model(x_data)\n",
    "            loss = loss_fn(logits, y_data)\n",
    "            \n",
    "            # 计算准确率\n",
    "            pred = paddle.argmax(logits, axis=1)\n",
    "            correct += (pred == y_data.flatten()).numpy().sum()\n",
    "            total += y_data.shape[0]\n",
    "            \n",
    "            # 累计验证损失\n",
    "            val_loss += loss.numpy()\n",
    "            val_batch_count += 1\n",
    "            \n",
    "    # 计算指标\n",
    "    avg_loss = total_loss / batch_count\n",
    "    avg_val_loss = val_loss / val_batch_count\n",
    "    acc = correct / total\n",
    "    \n",
    "    # 输出当前epoch信息\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}, Acc: {acc:.4f}, '\n",
    "          f'LR: {current_lr:.6f}, Time: {time.time()-start_time:.1f}s')\n",
    "\n",
    "    # 保存最优模型\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        paddle.save(model.state_dict(), f'{save_dir}/best_model.pdparams')\n",
    "        \n",
    "            \n",
    "        print(f'Best model saved: {best_acc:.4f}')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'No improvement. Patience: {patience_counter}/{patience}')\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    # 更新记录和图表\n",
    "    records['epochs'].append(epoch + 1)\n",
    "    records['train_losses'].append(float(avg_loss))\n",
    "    records['val_accs'].append(float(acc))\n",
    "    records['lrs'].append(float(current_lr))\n",
    "    clear_output(wait=True)\n",
    "    update_plots(records, lines, ax1, ax2, ax3)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dxy\\AppData\\Local\\Temp\\ipykernel_30876\\3522186234.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# 绘制最终结果图\n",
    "plt.ioff()\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 上半部分：损失和准确率\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(records['epochs'], records['train_losses'], 'b-', label='Loss')\n",
    "plt.title('loss&acc', fontsize=15)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "ax2.plot(records['epochs'], records['val_accs'], 'r-', label='Accuracy')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylim([0, 1.0])\n",
    "\n",
    "# 下半部分：学习率\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(records['epochs'], records['lrs'], 'g-')\n",
    "plt.title('lr', fontsize=15)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{save_dir}/training_curves_final.png')\n",
    "plt.show()\n",
    "\n",
    "# 清理显存\n",
    "paddle.device.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "    使用生成的模型进行预测\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集样本数: 394\n",
      "正确预测数: 390\n",
      "测试集准确率: 0.9898 (390/394)\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from paddle.vision import mobilenet_v3_small\n",
    "model_dict = paddle.load(\"./work/mymodel/best_model.pdparams\") #加载模型参数   \n",
    "model = mobilenet_v3_small(num_classes=num_classes, pretrained=True)\n",
    "model.load_dict(model_dict) #加载模型参数\n",
    "model.eval() \n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with paddle.no_grad():  \n",
    "    for x_data, y_data in loaders['test']():\n",
    "        # 前向传播\n",
    "        logits = model(x_data)\n",
    "        \n",
    "        # 计算准确率\n",
    "        pred = paddle.argmax(logits, axis=1)\n",
    "        correct += (pred == y_data.flatten()).numpy().sum()\n",
    "        total += y_data.shape[0]\n",
    "\n",
    "# 4. 计算最终准确率\n",
    "test_acc = correct / total\n",
    "\n",
    "# 5. 输出结果\n",
    "print(f\"测试集样本数: {total}\")\n",
    "print(f\"正确预测数: {correct}\")\n",
    "print(f\"测试集准确率: {test_acc:.4f} ({correct}/{total})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
